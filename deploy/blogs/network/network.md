# 因特网

当两个设备要通信时，计算机网络就被发明出来了。本文将梳理出网络通信的主要逻辑。

因特网是一个世界范围的计算机网络，它互联了全是界数十亿计算设备。所有这些设备都称为**主机**或者**端系统**。到2015年大约有50亿台设备接入因特网。这些端设备
**由网络层的路由器和链路层的交换机连接在一起**。所有的这些硬件都需要遵循一定的协议才能正常运行。

# 分组交换与电路交换

电路交换中，端系统通话期间预留通信所需要的资源，意味着在这期间它是独占这些资源的，打电话就是这种情况。

互联网主要使用的是另一种技术，分组交换，是所有用户共享资源的的，本文主要介绍这一种。

# 时延和丢包

分组从一台主机出发，通过一系列路由器，到达另一台主机，整个历程的耗时就称为时延。我们对时延进行分类，可以梳理出以下四种类型。

<div align="center">

<img src="/blogs/network/delay.png" height="50%" width="50%"></img>

</div>

- 处理时延

  路由器计算下一跳该转发到哪（三层功能），检查比特级别的差错（二层功能）所消耗的时间，该时延通常是微秒级别。

- 排队时延

  分组交换是所有用户共享的，比如当一个路由器接收到多个方向发送来的数据，都想转发到一个方向，**接收速度大于发送速度就要排队**，路由器内部有个缓存队列，
  会把接收到的数据包保存下来，等待队列前面的数据包发送完成，队列满了新来的数据包就会被丢弃。**程序员常说的丢包就是这种情况，但它不代表最终的目标端系统没有收到数据包，
  因为可能还有重发机制兜底。** 实际的排队延时通常是微秒或者毫秒级别。

- 传输时延

  这个延时取决于路由器把数字比特推送到链路的能力，这取决于网络链路，网卡的带宽。比如带宽是R=100Mbps，传输L比特的时延是L/R

- 传播时延

  比特在链路上传播所需要的时间，取决于物理媒体（光线或者、双胶铜线等）和传输距离。

**以上就是节点到节点之间的时延，端到端的时延就是所有节点间时延的总和**

# 吞吐量

吞吐量就是从端系统之间数据传输的速度。比如端系统接收L比特的数据用去了T秒，那么吞吐量就是L/T bps。

**两个端系统之间常常要经过多个路由器，吞吐量的上限取决于整个路程中，吞吐量最低的那两个节点。**

# 协议分层

<div align="center">

<img src="/blogs/network/network-layer.png" height="50%" width="50%"></img>

</div>
常见的分层模型有两种，5层模型和7层模型，后者只是把应用层更加细分了一下。

**对于程序员来说，底下的四层是比较固化的，大部分时候只需要考虑在传输层用tcp还是upd？在网络层用ipv4还是ipv6？链路层承载的最大载核是多少，我每次发送多少字节的数据才是最佳选择？**

应用层的情况就多种多样了，甚至程序员很容易的定制自己的协议，因为它仅仅只是两个端系统上的应用沟通的语言，互相能够理解就是一种可行的协议。

数据传输时，发送端从应用层开始，逐渐向下层传递，每一层对上一层传过来的数据加上本层的头部。最终在物理层通过以太网或者其他形式的网络接入发送出去。
接收方正好相反，数据逐层向上传递，每一层打开属于本层的头部进行相应处理。

**原则上不应该在某一层开不属于本层的数据报，NAT协议工作在网络层，负责网络地址转换，但是他却打开了传输层的报文查看端口号，这是关于NAT协议是否合理有争议的一点**

<div align="center">

<img src="/blogs/network/network-data.png" height="50%" width="50%"></img>

> 一个完整的待发送的数据报文

</div>

下面介绍一下各个层次中的重点

## 应用层

应用层最常用的就是http协议了，它定义了web应用之间的通信语言。

### http1.1和队头阻塞

http1.1是http协议的第一个标准版本。

请求报文

```shell
POST /user HTTP/1.1                       // 请求行
Host: www.user.com
Content-Type: application/x-www-form-urlencoded
Connection: Keep-Alive
User-agent: Mozilla/5.0.                  // 以上是请求头

hello world                               // 请求体
```

**需要注意的是请求头中的Connection: Keep-Alive，目的是通知接收方不要关闭TCP连接，我接下来还有数据要发，可以复用同一个tcp连接。Connection: close，
则正好相反，意在通知对方把连接关了吧。**

响应报文

```shell
HTTP/1.1 200 OK                           // 响应行
Connection: close
Content-Encoding: gzip
Content-Type: text/html
Transfer-Encoding: chunked                // 以上是响应头

<!DOCTYPE html>                           // 响应体
<!--STATUS OK-->
```

**在http1.1版本中，传输层用的是tcp协议，每一个tcp连接中，可以进行多次http请求，但是下一个请求必须等到上一个请求得到响应才能发出，这样往往会把问题扩大。这种情况叫做http队头阻塞。**

**http1.1之所以要等待收到响应才能发下一个请求，是因为如果不这样，它无法确定哪个响应对应哪个请求**

那么我多弄几个连接不就可以了吗？

确实可以解决这个问题，但是会引入新的问题，比如连接增多负载就会增高。所以浏览器都会对连接数上限有限制，比如chrome允许最多同时6个连接。

这个问题在http2.0中才彻底解决。

### http2.0

http2.0在语义上完全兼容http1.1，它改变的是传输层面的逻辑，提升性能。**同时http2对报文进行了哈夫曼编码，相比于ascii码，可以用少的0和1表示相同的数据。**

编码是它提升性能的方式之一，因为减少了数据传输总量。另一个方式就是通过解决上面提到的问题，队头阻塞。

编码后的http2报文长这样子。

```shell
+-----------------------------------------------+
|                 Length (24)                   |
+---------------+---------------+---------------+
|   Type (8)    |   Flags (8)   |
+-+-------------+---------------+-------------------------------+
|R|                 Stream Identifier (31)                      |
+=+=============================================================+
|                   Frame Payload (0...)                      ...
+---------------------------------------------------------------+
```

其中Stream Identifier叫做流标识符，它作为http2连接内唯一的id标识，通过这个值绑定了响应和请求1对1的关系。

### http3

至此，http2已经很好的解决了应用层面的问题了，实际上http3的推出，是为了解决传输层的问题。

http1.1和http2.0都是基于tcp协议的。tcp在传输层面上为了保证可靠，采取了重传，排序，拥塞控制，流量控制的策略，实现这些策略对性能是有牺牲的。
http3就舍弃了tcp，使用upd作为载体，udp没有这些保障，更单纯的传输数据，所以速度更快。这就要求应用必须自己保证可靠性了。
所以http3在udp上使用了**quic协议，用更高效的方式做tcp应该做的这些事。** 这里就不过多展开了。

## 传输层

传输层最常见的两个协议是tcp和udp。

### tcp

<div align="center">

<img src="/blogs/network/tcp-data.png" height="50%" width="50%"></img>

> tcp报文

</div>

首先其中的数据部分就是应用层交下来的完整的内容。

使用tcp协议在真正的业务通信之前，首先要建立连接。**所谓的建立连接并不是在端系统之间搭个桥，只是在双方端系统之间，创建出用于管理通信的数据结构。真的搭个桥的，那应该是昂贵的电路交换把。**

怎么建立连接的呢，通过三次握手，三次握手之后，双方的端系统里就有了一个四元组。

| socket | 源ip       | 源端口 | 目标ip      | 目标端口 |
| ------ | ---------- | ------ | ----------- | -------- |
| 1      | 30.99.22.1 | 443    | 22.22.11.22 | 33377    |

之后双放进程只需要知道socket是哪个就够了。

除此之外tcp报文中还有一个窗口大小，这个是用来做**流量控制**和**拥塞控制**的。

当接收端接收的慢，发送方发的快，接收方就会通过这个窗口通知发送方慢一点。**因为此时发再快也没有意义，还会占用核心网络资源，弄不好还会增加排队时间。**

当核心网络拥堵时，也会通过这个窗口来通知发送方慢一点。**避免长时间排队，甚至队列满了还会丢包。**

tcp是怎么保证对方一定能收到的？

tcp报文中有一个确认号，接收方对每一个收到的报文都会发送响应的应答包给发送方。**所以，只要超过了约定的时间还没收到应答，我就重发。**

最后通信完断开链接，还要经过四次挥手，这个不展开了。

### upd

udp在传输层上只做了一件事，就是校验数据宝是否完整（tcp也做了这件事）。其他的都没有，所以也不需要挥手，也不需要握手，所以数据包被路由器扔了他也不会重传。
适用于即时丢一些数据包也没关系的场景，但是速度要快。比如多媒体的传输。

## 网络层

数据传输过程中，传输路线是什么样的，我该先走哪个路由器再走哪个，这是网络层的工作。

每个路由器内都有一个路由表，它定义了目的ip与出口的映射关系。需要注意的是，**这个映射关系是按照子网的网络号匹配的。**

每个ip地址分成2个部分，比如192.168.1.1/24代表前24位是网络号，最后8位是主机号，路由器只需要交给正确的子网就可以了，这样可以减少路由表的长度。

### ipv4与ipv6

上面介绍的192.168.1.1/24就是ipv4的地址。因为地址数量不够用了，所以ipv6出现了，ipv6的长度是128位，再用十进制表示就太长了，所以用了16进制或者0压缩法表示。

除了这个，**ipv6是不允许分片的，这就要求上层要把控好数据传输的大小。** 什么是分片会在下面的链路层介绍。

### 怎么从ipv4过度到ipv6？

现在，全球绝大部分的路由设备都是按照的ipv4协议进行路由转发，两个设备必须使用相同的协议才能对话，ipv4和ipv6是无法对话的。由于数量太多，不可能一下子让所有路由设备都升级成ipv6。
所以只能在一个小的子网内部先用ipv6，对于这个子网的出口，他的外界全是ipv4，所以在出口处，**我们在ipv6的报文上再套上一层ipv4的头部，** 这样便能再核心网络中传播了。
等到这样的子网越来越多，最终整个核心网络都是ipv6的子网，那就可以彻底和ipv4说再见了👋。

## 链路层

链路层定义了连个节点之间该怎么传数据。

首先链路层是可靠的，**但是这里的可靠和tcp的可靠不是一回事，链路层保证的是我从这一个节点一定准确传到了下一个节点，否则就重传。至于被路由器扔了，那我是不管的
，我已经准确传过去了，路由器扔了那是网络层的事。**

另一个关键点是mtu，mtu定义了链路层载荷的大小，也就是网络层头+传输层头+应用层数据的大小，超过了这个大小就要分片传输了。

所以一个程序优化的关键点是，**尽量在一个mtu的大小内完成更多的业务目的。**

另外，每个路由器的mtu都可能不同，一次数据传输可能要经历多个路由器，所以**找出整个链路最小mtu是另一个关键点。**

## 物理层

从软件开发的角度来说，物理层会接触的很少，还有把有限的精力放在更常用的事情上吧。😄

# 总结

以上就是对数据在计算机网络中传输的一个概括，还有很多细节并没有提到，比如拥塞控制的算法，tcp的慢启动，路由算法，广播风暴等等，需要你自行去探索了。

但是先有一个框架是很好的，因为每一层网络都是环环相扣，**不对整体有所了解，就不可能真正掌握局部。**
