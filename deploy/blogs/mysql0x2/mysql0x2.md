# mysql 日志

索引的意义总结为一句话就是为了提高查询性能。但是一个出色的产品光有性能是不够的，必须还能稳定安全的工作，本文介绍的内容关乎mysql的稳定和安全。

mysql有两个重要的日志模块，工作在server层的binlog和innoDB特有的redo log。

## redo log

对于写操作，如果每一次都要找到磁盘中对应的那条记录再写进去，速度会非常慢。mysql里应用了WAL（Write-Ahead Logging）技术，它的关键点就是先写日志，再写磁盘。

对于写操作，InnoDB引擎先把记录写到redo log 然后更新内存，写操作就完成了。等系统不忙的时候再把数据写到磁盘里。

**redo log也是在磁盘中，但是它是顺序读写，修改InnoDB的数据是随机读写，速度相差较大。**

**除了性能，redo log也保证了，当mysql服务宕机了，内存中那些还没有刷入磁盘的数据丢失了，也可以通过redo log恢复出来(crash-safe)**

### 数据刷盘

通过redo log 的工作机制，我们可以看到，mysql会在redo log 满了或者 内存不够用了的时候，把内存中的数据刷入磁盘，这样就可以随时释放内存，同时删掉相应的redo log。对于**内存中还没刷入磁盘的部分称为脏页，已经刷入磁盘的部分叫做干净页。**
除了redo log 满了和内存不够了，还有mysql正常关闭时会刷盘，系统空闲时会刷盘。后两种情况并不需要额外关注，前两种情况有可能会出现性能问题。

所以**我们需要控制每次刷脏页的策略，避免一次刷太多导致性能下降严重。**

参数innodb_io_capacity控制着刷脏页的速度，它的值要参考两个方面，磁盘的io性能和脏页比例。
**如果要全力保证数据库刷脏页的性能就把innodb_io_capacity设置为磁盘的IOPS**

可以使用fio测试磁盘的iops

```shell
fio --randrepeat=1 --ioengine=posixaio --direct=1 --gtod_reduce=1 --name=test --filename=test --bs=4k --iodepth=64 --size=4G --readwrite=randrw --rwmixread=75
```

**参数 innodb_flush_neighbors 控制着刷脏页时是否把相邻的脏页一起刷进去，对于机械磁盘建议开启，SSD建议关闭**

## binlog

binlog是Mysql sever层维护的一种二进制日志，其主要是用来记录对mysql数据更新或潜在发生更新的SQL语句，并以"事务"的形式保存在磁盘中。

主要作用有2个

- **用于主从同步，slave执行master的binlog**
- **数据恢复，全量备份+binlog可以恢复到覆盖范围内的任意一秒**

binlog有三种格式

- Statement
  - Statement 模式只记录执行的 SQL，不需要记录每一行数据的变化，因此极大的减少了 binlog 的日志量，避免了大量的 IO 操作，提升了系统的性能。但是，正是由于 Statement 模式只记录 SQL，而如果一些 SQL 中包含了函数，那么可能会出现执行结果不一致的情况。比如说 uuid() 函数，每次执行的时候都会生成一个随机字符串，在 master 中记录了 uuid，当同步到 slave 之后，再次执行，就获取到另外一个结果了。
- Row
  - Row 格式不记录 SQL 语句上下文相关信息，仅仅只需要记录某一条记录被修改成什么样子了。Row 格式的日志内容会非常清楚的记录下每一行数据修改的细节，这样就不会出现 Statement 中存在的那种数据无法被正常复制的情况。不过 Row 格式也有一个很大的问题，那就是日志量太大了，特别是批量 update、整表 delete、alter 表等操作，由于要记录每一行数据的变化，此时会产生大量的日志，大量的日志也会带来 IO 性能问题。
- Mixed
  - 这种格式实际上就是 Statement 与 Row 的结合。在 Mixed 模式下，系统会自动判断该用 Statement 还是 Row：一般的语句修改使用 Statement 格式保存 binlog;对于一些 Statement 无法准确完成主从复制的操作，则采用 Row 格式保存 binlog。

## 两阶段提交

为了保证binlog和redo log逻辑相同，所采取的一种机制。

第一次写入redo log处于prepare状态，然后生成binlog并写入磁盘，最后把redo log状态设置为commit。

- 当prepare log完成了，binlog还没写入mysql crash了，重启恢复会回滚掉这条prepare log。备份恢复由于没有binlog，也不会存在这条日志，逻辑一致。
- 当prepare log和binlog都完成了，此时mysql crash了，虽然没有commit，但是mysql重启后会自动commit。备份恢复有binlog，逻辑一致。

## 注意点

- **innodb_flush_log_at_trx_commit这个参数设置为1，保证每个事务都redo log都会写入磁盘，获得可靠的crash-safe能力。**
- **sync_binlog这个参数设置为1，保证每个事务的binlog都会写入磁盘，保证binlog的crash-safe。**

# 事务

事务的目的就是为了保证一组mysql操作要么全成功，要么全失败。

为了达到目的，事务需要具备4个特性原子性、一致性、隔离性、持久性（Atomicity、Consistency、Isolation、Durability，即ACID）。

## 隔离性

当多个事务同时工作时，可能会出现脏读（读到了其他未提交事务的修改数据），不可重复读（同一个事务中，前后读取结果不同），幻读（同一个事务中，前后读取的行数不同）的问题。

为了解决这些问题，mysql支持4个级别的隔离性。这4种隔离级别，并行性能依次降低，安全性依次提高。

- 读未提交：别人改数据的事务尚未提交，我在我的事务中也能读到。
- 读已提交：别人改数据的事务已经提交，我在我的事务中才能读到。
- 可重复读：别人改数据的事务已经提交，我在我的事务中也不去读（如果我开启事务时他还没提交）。
- 串行：我的事务尚未提交，别人就别想改数据。

**可重复读(RR)隔离界别可以理解为在事务启动时对数据库拍了一个快照，后续的查询都是基于这个快照的，所以mysqldump的逻辑备份就是在RR下执行的。**

### 隔离的实现原理

mysql的写操作，除了记录变更记录，还会记录一条变更相反的回滚操作记录，前者记录在redo log，后者记录在undo log。

只有当没有任何事务需要这条undo log时，他才会被删除。所以**避免长事务的一个重要的理由是避免undo log堆积过多。 另一个理由是占有锁资源。**

### 使用事务

启动事务有两种方式

- **begin 或 start transaction 在下一条sql执行时启动事务，start transaction with consistent snapshot表示立即启动事务**。commit提交结束事务，或者rollback结束事务。begin 和 start transaction是等价的
- set autocommit=0，意味着如果你只执行一个 select 语句，这个事务就启动了，而且并不会自动提交。这个事务持续存在直到你主动执行 commit 或 rollback 语句，或者断开连接。**事务未提交时断开链接，逻辑上相当于rollback**

**commit work and chain,相比于 commit 不仅会提交事务，还会自动开启下一个事务。可以节约执行一次begin的开销。**

# 锁

为了处理控制并发访问，我们需要用到锁。根据加锁的范围，可以分为全局锁，表级锁，行锁。

## 全局锁

mysql具备一个全局读锁，通过命令Flush tables with read lock开启全局读锁，整个数据库会进入只读状态，通常在**逻辑备份的时候开启全局读锁，避免备份过程中出现逻辑不一致。**

对于支持事务的 InnoDB 也可以通过设置可重复读的隔离级别，来屏蔽其他事务的写操作，从而达到逻辑一致。

**mysqldump 使用参数–single-transaction，可以利用事务的隔离性保证逻辑备份的逻辑一致性，代替全局读锁。**

## 表级锁

表级锁有两种，表锁和元数据锁（meta data lock,MDL）

**加表锁的语法是 lock tables … read/write，解除语法是unlock tables， 读锁是共享锁，写锁是互斥锁。**

MDL锁是server层的锁，表级锁，主要用于隔离DML和DDL操作之间的干扰。**每执行一条DML、DDL语句时都会申请MDL锁，DML操作需要MDL读锁，DDL操作需要MDL写锁，MDL读锁不会影响dml写操作**

**在给热点表进行字段变更时需要加mdl写锁，这意味着其他线程的所有操作都会进入等待，所以一定要设置超时时间自动释放锁。**

## 行锁

如果用户更新某一个行导致整张表都被锁住了，代价还是比较高的。有了行锁，我们就可以只对这一行加锁。

**在InnoDB事务中，行锁是在需要时加上的，等事务结束后释放。** 所以，**如果事务中需要锁多个行，要把最可能造成冲突的锁尽可能放在后面，减少锁住时间。**

**RR隔离级别下行锁锁住的是扫描的所有行而不是目标行，所以如果没有索引的话会锁住全表，并且更新语句会顺便把主键索引上满足条件的行加上行锁。**

**RC隔离级别下会短暂的锁住扫描的行，然后不满足条件的行解锁，满足条件的行提交事物时解锁。**

**在RC隔离级别下由于没锁住扫描到的所有行，这可能会导致binlog执行时序错误，这种时候需要把binlog设置为row格式。**

**锁是加在索引上的，如果你要用 lock in share mode 来给行加读锁避免数据被更新的话，就必须得绕过覆盖索引的优化，在查询字段中加入索引中不存在的字段。**

## 间隙锁

在RR隔离级别下，如果只锁住扫描的所有行，其他事务还是能插入数据的，因为不存在的行并没有上锁。如果插入的这一行的事务先提交了，还是存在binlog时序的问题。所以InnoDB还启用了间隙锁，**间隙锁锁住的是插入动作，范围是行锁两边的空隙。**从而解决了幻读问题。

但是如下图所示**间隙锁会造成死锁，影响并发度。**

<div align="center">
<img src="/blogs/mysql0x2/gap-lock.png" height="40%" width="40%"></img>

> 间隙锁导致的死锁

</div>

### 死锁

<div align="center">
<img src="/blogs/mysql0x2/dead-lock.png" height="40%" width="40%"></img>

> 两个事务发生死锁

</div>

图中事务A的最后一条SQL在等待事务B释放锁，事务B的最后一条SQL在等事务A释放锁，于是谁都无法执行了。

对于死锁，有两种解决方式。

- **参数 innodb_lock_wait_timeout 设置锁超时，不管是不是死锁，到了时间没执行完就回滚，从而释放锁。**
- **参数 innodb_deadlock_detect 设置为 on ，开启死锁检测，发现死锁后自动回滚其中一个事务，从而释放锁。**

如果使用超时控制的话，要估算一个合适的超时时间，不能让死锁锁住太久，也不能让普通的锁等待被误杀。这个时长并不是很容易找到的，所以第二种方式更长用。

死锁的判断逻辑是这样的，**当一个事务被锁的时候，就要看看它所依赖的线程有没有被别人锁住，如此循环，最后判断是否出现了循环等待，也就是死锁。**

但是死锁检测也有一个弊端，就是它非常消耗CPU。当1000个线程同时更新同一行，死锁检测操作就是100万的数量级。有两个思路优化这个问题。

- **在业务上保证不会出现死锁，这样就可以关闭死锁检测的功能了。**
- **并发度控制，让同时操作同一行的线程数量别太多，避免大量的死锁检测。**

### 再看可重复读

前面我们提到过，对于可重复读的隔离级别，我的事务整个过程中读到的值应该是完全相同的。但是又因为行锁，事务在执行过程中可能需要等待别的事务更新同一行，那这时候如果该事务也要更新，看到的还是事务刚刚创建时事务的状态吗？ 显然不是，这里的关键就是**对于更新操作来说，即使是可重复读的隔离级别也需要当前读。**

#### 快照

在可重复读隔离界别下，事务在启动时就拍了个基于整库的快照。

这个快照并不是把整个库复制一遍，那样就不可能快了。innoDB中每个事务都有一个transaction id，每行数据有多个版本，每个版本对应一个row trx_id。当事务修改这一行后，就会记录一个版本，将row trx_id设置为 transaction id。**undo log记录的就是从新的row trx_id版本到上一个row trx_id的回滚方法，所以数据库并不需要记录每一个版本的数据，而是用新版本和undo log 计算出旧版本。**

所以**可重复读的这个快照只需要承认在他启动之前的最新的版本就可以了，同时如果某一行是他自己修改后生成了新版本，那也需要承认。**

# 总结

- 如果要全力保证数据库刷脏页的性能就把innodb_io_capacity设置为磁盘的IOPS
- 参数 innodb_flush_neighbors 控制着刷脏页时是否把相邻的脏页一起刷进去，对于机械磁盘建议开启，SSD建议关闭
- binlog主要有两个作用，用于主从同步，slave执行master的binlog。数据恢复，全量备份+binlog可以恢复到覆盖范围内的任意一秒。
- innodb_flush_log_at_trx_commit这个参数设置为1，保证每个事务都redo log都会写入磁盘，获得可靠的crash-safe能力。
- sync_binlog这个参数设置为1，保证每个事务的binlog都会写入磁盘，保证binlog的crash-safe。
- 避免长事务的一个重要的理由是避免undo log堆积过多。 另一个理由是占有锁资源。
- 可重复读(RR)隔离界别可以理解为在事务启动时对数据库拍了一个快照，后续的查询都是基于这个快照的，所以mysqldump的逻辑备份就是在RR下执行的。
- RR隔离级别下行锁锁住的是扫描的所有行而不是目标行，所以如果没有索引的话会锁住全表，并且更新语句会顺便把主键索引上满足条件的行加上行锁。
- RR隔离级别下使用间隙锁解决了幻读问题，但是间隙锁会造成死锁，影响并发度。
- 锁是加在索引上的，如果你要用 lock in share mode 来给行加读锁避免数据被更新的话，就必须得绕过覆盖索引的优化，在查询字段中加入索引中不存在的字段。
- RC隔离级别下会短暂的锁住扫描的行，然后不满足条件的行解锁，满足条件的行提交事物时解锁。
- 在RC隔离级别下由于没锁住扫描到的所有行，这可能会导致binlog执行时序错误，这种时候需要把binlog设置为row格式。
- 参数 innodb_lock_wait_timeout 设置锁超时，不管是不是死锁，到了时间没执行完就回滚，从而释放锁。
- 参数 innodb_deadlock_detect 设置为 on ，开启死锁检测，发现死锁后自动回滚其中一个事务，从而释放锁。
- 通过并发度控制，让同时操作同一行的线程数量别太多，避免大量的死锁检测
- 对于更新操作来说，即使是可重复读的隔离级别也需要当前读。
- 在给热点表进行字段变更时需要加mdl写锁，这意味着其他线程的所有操作都会进入等待，所以一定要设置超时时间自动释放锁。
